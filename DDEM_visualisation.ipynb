{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6002b8ee-313a-44e4-98eb-180d49419ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiona\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('nbagg')\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import HBox, Label, IntSlider,IntRangeSlider\n",
    "from ipywidgets import HTML\n",
    "from IPython.display import clear_output\n",
    "from ipyleaflet import Map, basemaps, basemap_to_tiles, CircleMarker, Popup, Polyline, Polygon, LayerGroup, LayersControl,ScaleControl, FullScreenControl\n",
    "from ipyleaflet import WidgetControl\n",
    "from ipywidgets import interactive\n",
    "import matplotlib as mpl\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, Label, IntSlider,IntRangeSlider\n",
    "#import matplotlib.patches as patches\n",
    "#from matplotlib.patches import Polygon\n",
    "import matplotlib\n",
    "import matplotlib.colors\n",
    "import contextily as cx\n",
    "import shapely.geometry as sg\n",
    "from rijksdriehoek import rijksdriehoek\n",
    "from matplotlib.collections import PatchCollection\n",
    "import ipympl\n",
    "import warnings\n",
    "import numpy as np\n",
    "import ast\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "RD = rijksdriehoek.Rijksdriehoek()\n",
    "plt.rcParams.update({'font.size': 24})\n",
    "MAX_GROUP_SIZE = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39f726a-b4b9-4400-bac7-a1c37154002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gpkg(filename):\n",
    "    \n",
    "    assert '.' in filename, f'Filename {filename} does not have an extension!'    \n",
    "    if filename.split('.')[1] != 'gpkg':\n",
    "        raise ValueError(f'Expected gpkg, got {filename}')\n",
    "    if not os.path.isfile(f'Data/{filename}'):\n",
    "        raise ValueError(f'File {filename} not found in Data directory!')\n",
    "    \n",
    "    AOI = {}\n",
    "    with fiona.open(f\"Data/{filename}\") as layer:\n",
    "        for feature in layer:     \n",
    "            AOI[feature['properties']['ogc_fid']] = {\n",
    "                \"crd_dec\": [feature['geometry']['coordinates'][ii] for ii in range(len(feature['geometry']['coordinates']))]\n",
    "                }\n",
    "            for key in ['h0', 's_h0', 'xI', 's_xI', 'xP', 's_xP', 'xE', 's_xE', 'tau', 's_tau',\n",
    "                        'rho_xIh0', 'rho_xPh0', 'rho_xEh0', 'rho_tauh0',\n",
    "                        'rho_xPxI', 'rho_xExI', 'rho_xItau', \n",
    "                        'rho_xPxE', 'rho_xPtau', \n",
    "                        'rho_xEtau', 'xI_frac']:\n",
    "                AOI[feature['properties']['ogc_fid']][key] = feature['properties'][key]\n",
    "    return AOI\n",
    "\n",
    "def read_visualisation_gpkg(filename):\n",
    "    assert '.' in filename, f'Filename {filename} does not have an extension!'    \n",
    "    if filename.split('.')[1] != 'gpkg':\n",
    "        raise ValueError(f'Expected gpkg, got {filename}')\n",
    "    if not os.path.isfile(f'Data/{filename}'):\n",
    "        raise ValueError(f'File {filename} not found in Data directory!')\n",
    "    \n",
    "    AOI = {}\n",
    "    with fiona.open(f\"Data/{filename}\") as layer:\n",
    "        for feature in layer:     \n",
    "            AOI[feature['properties']['ogc_fid']] = {\n",
    "                \"crd_dec\": [feature['geometry']['coordinates'][ii] for ii in range(len(feature['geometry']['coordinates']))]\n",
    "                }\n",
    "            for key in ['h2015_m', 'sigma_h2015_m', #'h2023_m', 'v_mpy', 'sigma_v_mpy', \n",
    "                   'xI_mpy', 'sigma_xI_mpy', 'xI_frac_1000pct', \n",
    "                   'mean_yearly_irreversible_subsidence_MYIS_mpy', 'sigma_MYIS_mpy',\n",
    "                   'xP_mpmm', 'sigma_xP_mpmm', 'xE_mpmm', 'sigma_xE_mpmm', 'tau_kd', 'sigma_tau_kd',\n",
    "                   'rho_h2015_xI_1000', 'rho_h2015_xP_1000', 'rho_h2015_xE_1000', 'rho_h2015_tau_1000',\n",
    "                   'rho_xI_xP_1000', 'rho_xI_xE_1000', 'rho_xI_tau_1000', \n",
    "                   'rho_xP_xE_1000', 'rho_xP_tau_1000', \n",
    "                   'rho_xE_tau_1000',\n",
    "                   'timespan_ky', 'n_observations_1000', \n",
    "                   'significant_irreversible_subsidence_detected_2s_1000', 'significant_irreversible_subsidence_detected_3s_1000', \n",
    "                   'discriminatory_power_2s_1000pct', \n",
    "                   'discriminatory_power_3s_1000pct', 'MDD_80p_mpy', \n",
    "                   'cum_irreversible_subsidence_since_2000_m', 'oldest_observation_ky', \n",
    "                   'OMT_sust_2s_1000', 'OMT_sust_3s_1000']:\n",
    "                AOI[feature['properties']['ogc_fid']][key] = feature['properties'][key]\n",
    "    return AOI\n",
    "\n",
    "def convert_wgs84_to_rd(list_of_coords):\n",
    "    \"\"\"\n",
    "    Converts a list of coordinates in WGS84 to RD\n",
    "    :param list_of_coords: list of (lon, lat) objects\n",
    "    :return: list of (rd_x, rd_y) objects\n",
    "    \"\"\"\n",
    "    rd_coords = []\n",
    "    for coord in list_of_coords:\n",
    "        RD.from_wgs(lon=coord[0], lat=coord[1])\n",
    "        rd_coords.append([RD.rd_x, RD.rd_y])\n",
    "    return rd_coords\n",
    "\n",
    "def convert_rd_to_wgs84(list_of_coords):\n",
    "    \"\"\"\n",
    "    Converts a list of coordinates in RD to WGS\n",
    "    :param list_of_coords: list of (rdx, rdy) objects\n",
    "    :return: list of (lon, lat) objects\n",
    "    \"\"\"\n",
    "    wgs_coords = []\n",
    "    for coord in list_of_coords:\n",
    "        RD.rd_x = coord[0]\n",
    "        RD.rd_y = coord[1]\n",
    "        wgs_coords.append(RD.to_wgs())\n",
    "    return wgs_coords\n",
    "\n",
    "def get_yearfrac(date):\n",
    "    year = eval(date[:4])\n",
    "    month = eval(date[5:7].lstrip('0'))\n",
    "    day = eval(date[8:].lstrip('0'))\n",
    "    dayOfYear = dt.datetime(year=year, month=month, day=day) - dt.datetime(year=year, month=1, day=1)\n",
    "    yearDays = dt.datetime(year=year+1, month=1, day=1) - dt.datetime(year=year, month=1, day=1)\n",
    "    yearfrac = year + dayOfYear.days / yearDays.days\n",
    "    return yearfrac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2355ef-cbc1-4a2b-903a-2c6de1c018a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_aoi():\n",
    "    filename = \"Data/AoI_GreenHeart-polygon.shp\"\n",
    "    shape = fiona.open(filename)\n",
    "    finished = False\n",
    "    shape_iter = iter(shape)\n",
    "    aoi_rd_poly = None\n",
    "    while not finished:\n",
    "        try:\n",
    "            shp = next(shape_iter)\n",
    "            polygons = [sg.Polygon(convert_wgs84_to_rd(shp[\"geometry\"][\"coordinates\"][ii])) for ii in range(len(shp[\"geometry\"][\"coordinates\"]))]\n",
    "            if len(polygons) > 1 or aoi_rd_poly is not None:\n",
    "                raise ValueError(\"Cannot handle AOI with more than 1 polygon!\")\n",
    "            aoi_rd_poly = polygons[0]\n",
    "            if not aoi_rd_poly.is_valid:\n",
    "                raise ValueError(\"Invalid polygon AOI!\")\n",
    "        except StopIteration:\n",
    "            finished = True\n",
    "    if aoi_rd_poly is None:\n",
    "        raise ValueError(f\"No valid AOI polygon found in {filename}!\")\n",
    "    return convert_rd_to_wgs84(np.array(aoi_rd_poly.exterior.coords.xy).T)\n",
    "aoi_wgs_poly = read_aoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f95202-077a-4bdb-846b-748674bfbbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_KNMI_file(filename):\n",
    "    f = open(filename)\n",
    "    data = f.read().split('\\n')\n",
    "    f.close()\n",
    "    start_idx = None\n",
    "    for i in range(len(data)):\n",
    "        if '# STN,YYYYMMDD' in data[i]:\n",
    "            start_idx = i + 2\n",
    "            break\n",
    "    headers = data[start_idx-2].split(',')\n",
    "    headers = [h.strip() for h in headers]\n",
    "    evapo_id = headers.index('EV24')\n",
    "    date_id = headers.index('YYYYMMDD')\n",
    "    precip_id = headers.index('RH')\n",
    "    temp_id = headers.index('TX')\n",
    "    processed_data = {}\n",
    "    for row in data[start_idx:]:\n",
    "        if row != '':\n",
    "            split_row = row.split(',')\n",
    "            date = dt.datetime.strptime(split_row[date_id].strip(), '%Y%m%d')\n",
    "            if '' not in [date, split_row[evapo_id].strip(), split_row[precip_id].strip(), split_row[temp_id].strip()]:\n",
    "                if '348' in filename and date.year < 1988: # very incomplete data before 1988\n",
    "                    continue\n",
    "                processed_data[date] = [eval(split_row[precip_id].strip()),\n",
    "                                        eval(split_row[evapo_id].strip()),\n",
    "                                        eval(split_row[temp_id].strip())]\n",
    "                if processed_data[date][0] == -1:\n",
    "                    processed_data[date][0] = 0\n",
    "                processed_data[date][0] /= 10\n",
    "                processed_data[date][1] /= 10\n",
    "                processed_data[date][2] /= 10\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7609a-1fce-45a8-be3f-b665b74ec872",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpkg_data = read_gpkg('InSAR_ALS_GNSS_Grav_Leveling.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4608fda5-4223-4dcb-bc57-8e8cae8603b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_data = read_visualisation_gpkg('Visualisation_export.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d770b3-2b68-47b0-a216-5038934c67db",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = '2015-01-02'\n",
    "Cabauw = read_KNMI_file(f'Data/etmgeg_348.txt')\n",
    "tmax = max(Cabauw.keys())\n",
    "tmax = '{:0>4d}-{:0>2d}-{:0>2d}'.format(tmax.year, tmax.month, tmax.day)\n",
    "tmin = min(Cabauw.keys()) + dt.timedelta(days=365) # to give space for tau to run back\n",
    "tmin = '{:0>4d}-{:0>2d}-{:0>2d}'.format(tmin.year, tmin.month, tmin.day)\n",
    "etmgeg = Cabauw \n",
    "\n",
    "tmin_yf = get_yearfrac(tmin)\n",
    "tmax_yf = get_yearfrac(tmax)\n",
    "t0_yf = get_yearfrac(t0)\n",
    "t0_dt = dt.datetime.strptime(t0, '%Y-%m-%d')\n",
    "tmin_dt = dt.datetime.strptime(tmin, '%Y-%m-%d')\n",
    "tmax_dt = dt.datetime.strptime(tmax, '%Y-%m-%d')\n",
    "\n",
    "t_eval_min = dt.datetime(1960, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356e9f2-d470-4874-b566-c048d0eb2c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_SPAMS_extended(EUP, t0, tmin, tmax, force_noInSAR=False):\n",
    "    # print(EUP)\n",
    "    InSAR = False\n",
    "    if EUP['InSAR'] != -999 and not force_noInSAR:\n",
    "        InSAR = True\n",
    "        \n",
    "    def no_insar_model():\n",
    "        t0_yf = get_yearfrac(t0)\n",
    "        t_data = []\n",
    "        s_data = []\n",
    "        h_data = []\n",
    "        if EUP['EUP_type'] == 'brp':\n",
    "            for idx in ['ThMD', 'AHN1', 'AHN2', 'AHN3', 'AHN4']:\n",
    "                if idx != 'ThMD':\n",
    "                    if -999 not in [EUP[f'{q}100_{idx}'] for q in 'hst'] and '' not in [EUP[f'{q}100_{idx}'] for q in 'hst']:\n",
    "                        h_data.append(EUP[f'h100_{idx}'])\n",
    "                        s_data.append(EUP[f's100_{idx}'])\n",
    "                        t_data.append(get_yearfrac(EUP[f't100_{idx}']))\n",
    "                else:\n",
    "                    if -999 not in [EUP[f'{q}_{idx}'] for q in 'hst'] and '' not in [EUP[f'{q}_{idx}'] for q in 'hst']:\n",
    "                        h_data.append(EUP[f'h_{idx}'])\n",
    "                        s_data.append(EUP[f's_{idx}'])\n",
    "                        t_data.append(get_yearfrac(EUP[f't_{idx}']))\n",
    "                                                    \n",
    "        else:\n",
    "            for idx in ['ThMD', 'AHN1', 'AHN2', 'AHN3', 'AHN4']:\n",
    "                if idx != 'ThMD':\n",
    "                    if -999 not in [EUP[f'{q}100_{idx}'] for q in 'hst'] and '' not in [EUP[f'{q}100_{idx}'] for q in 'hst']:\n",
    "                        h_data.append(EUP[f'h100_{idx}'])\n",
    "                        s_data.append(EUP[f's100_{idx}'])\n",
    "                        t_data.append(get_yearfrac(EUP[f't100_{idx}']))\n",
    "                else:\n",
    "                    if -999 not in [EUP[f'{q}_{idx}'] for q in 'hst'] and '' not in [EUP[f'{q}_{idx}'] for q in 'hst']:\n",
    "                        h_data.append(EUP[f'h_{idx}'])\n",
    "                        s_data.append(EUP[f's_{idx}'])\n",
    "                        t_data.append(get_yearfrac(EUP[f't_{idx}']))\n",
    "                        \n",
    "        if len(h_data) < 2:\n",
    "            return ('', (-999, -999), 0, [[-999, -999], [-999, -999]])\n",
    "        \n",
    "        time_basis = max(t_data) - min(t_data)\n",
    "        \n",
    "        y = np.zeros((len(t_data), 1))\n",
    "        A = np.ones((len(t_data), 2))\n",
    "        W = np.zeros((len(t_data), len(t_data)))\n",
    "        \n",
    "        for i in range(len(t_data)):\n",
    "            y[i, 0] = h_data[i]\n",
    "            A[i, 0] = (t_data[i] - t0_yf)/1000\n",
    "            if s_data[i] > 1e-8:\n",
    "                W[i, i] = 1/s_data[i]**2\n",
    "            else:\n",
    "                s = max(s_data)\n",
    "                if s > 1e-8:\n",
    "                    W[i, i] = 1/s**2\n",
    "                else:\n",
    "                    W[i, i] = 1 \n",
    "                    \n",
    "        Pa = np.linalg.inv(A.T @ W @ A) @ A.T @ W\n",
    "        sol = Pa @ y\n",
    "        Qy = Pa @ np.linalg.inv(W) @ Pa.T\n",
    "        \n",
    "        EoM_est = f\"h={round(sol[0][0], 4)}(t-t0)/1000+{round(sol[1][0], 4)}\"\n",
    "        vel_uncertainty = np.sqrt(Qy[0,0])\n",
    "        h_uncertainty = np.sqrt(Qy[1,1])\n",
    "    \n",
    "        return (EoM_est, (vel_uncertainty, h_uncertainty), time_basis, Qy)\n",
    "    \n",
    "    if InSAR: # has to be brp\n",
    "        initial_params = None\n",
    "        if 'xI' in EUP.keys() and 'xE' in EUP.keys() and 'xP' in EUP.keys() and 'tau' in EUP.keys() and 'h0' in EUP.keys():\n",
    "            if EUP['xI'] not in [None, -999]:\n",
    "                if EUP['xE'] not in [None, -999]:\n",
    "                    if EUP['xP'] not in [None, -999]:\n",
    "                        if EUP['h0'] not in [None, -999]:\n",
    "                            if EUP['tau'] not in [None, -999]:\n",
    "                                initial_params = [EUP['xP'], EUP['xE'], EUP['xI'], EUP['tau'], EUP['h0']]\n",
    "        if initial_params is None:\n",
    "            for idx in [37, 88, 110, 161]:\n",
    "                if f'p_{idx}' in EUP['InSAR'].keys():\n",
    "                    initial_params = list(EUP['InSAR'][f'p_{idx}'])\n",
    "                    initial_params.append(EUP['h100_AHN3']) # closest to t0, ~2015\n",
    "                    break\n",
    "\n",
    "            if initial_params is None:\n",
    "                initial_params = [np.nan] * 5\n",
    "        if None in initial_params:\n",
    "            for idx in [37, 88, 110, 161]:\n",
    "                if f'p_{idx}' in EUP['InSAR'].keys():\n",
    "                    initial_params = list(EUP['InSAR'][f'p_{idx}'])\n",
    "                    initial_params.append(EUP['h100_AHN3']) # closest to t0, ~2015\n",
    "                    break\n",
    "\n",
    "            if initial_params is None:\n",
    "                initial_params = [np.nan] * 5            \n",
    "        #tau = initial_params[3]\n",
    "        #_ = initial_params.pop(3)\n",
    "        \n",
    "        tmin_yf = get_yearfrac(tmin)\n",
    "        tmax_yf = get_yearfrac(tmax)\n",
    "        t0_yf = get_yearfrac(t0)\n",
    "        t0_dt = dt.datetime.strptime(t0, '%Y-%m-%d')\n",
    "        tmin_dt = dt.datetime.strptime(tmin, '%Y-%m-%d')\n",
    "        tmax_dt = dt.datetime.strptime(tmax, '%Y-%m-%d')\n",
    "        t_data = []\n",
    "        s_data = []\n",
    "        h_data = []\n",
    "        for idx in ['ThMD', 'AHN1', 'AHN2', 'AHN3', 'AHN4']:\n",
    "            if idx != 'ThMD':\n",
    "                if -999 not in [EUP[f'{q}100_{idx}'] for q in 'hst'] and '' not in [EUP[f'{q}100_{idx}'] for q in 'hst']:\n",
    "                    h_data.append(EUP[f'h100_{idx}'])\n",
    "                    s_data.append(EUP[f's100_{idx}'])\n",
    "                    t_data.append(get_yearfrac(EUP[f't100_{idx}']))\n",
    "            else:\n",
    "                if -999 not in [EUP[f'{q}_{idx}'] for q in 'hst'] and '' not in [EUP[f'{q}_{idx}'] for q in 'hst']:\n",
    "                    h_data.append(EUP[f'h_{idx}'])\n",
    "                    s_data.append(EUP[f's_{idx}'])\n",
    "                    t_data.append(get_yearfrac(EUP[f't_{idx}']))\n",
    "        InSAR_h = []\n",
    "        InSAR_t = []\n",
    "        InSAR_s = []\n",
    "        for idx in [37, 88, 110, 161]:\n",
    "            if f'h_{idx}' in EUP['InSAR'].keys():\n",
    "                for i in range(np.array(EUP['InSAR'][f'h_{idx}']).shape[0]):\n",
    "                    if not np.isnan(EUP['InSAR'][f'h_{idx}'][i]) and type(EUP['InSAR'][f't_{idx}'][i]) == str and not np.isnan(EUP['InSAR'][f's_{idx}'][i]):\n",
    "                        InSAR_h.append(EUP['InSAR'][f'h_{idx}'][i])\n",
    "                        InSAR_t.append(EUP['InSAR'][f't_{idx}'][i])\n",
    "                        InSAR_s.append(EUP['InSAR'][f's_{idx}'][i])\n",
    "        t_obs = []\n",
    "        y_obs = []\n",
    "        s_obs = []\n",
    "        for i in range(len(h_data)):\n",
    "            y_obs.append(h_data[i])\n",
    "            t_obs.append(t_data[i])\n",
    "            s_obs.append(s_data[i])\n",
    "        for i in range(len(InSAR_t)):\n",
    "            y_obs.append(InSAR_h[i])\n",
    "            t_obs.append(get_yearfrac(InSAR_t[i]))\n",
    "            s_obs.append(InSAR_s[i]) # 1 cm\n",
    "        \n",
    "        if t_data == []: # cannot estimate h0\n",
    "            boundaries = [[0, 0.1], #xP\n",
    "                          [0, 0.1], #xE\n",
    "                          [-0.1, 0], #xI\n",
    "                          [1, 365]]#, # tau\n",
    "                          #[-10, 100]]# h0\n",
    "            initial_params = initial_params[:-1]\n",
    "        else:\n",
    "            boundaries = [[0, 0.1], #xP\n",
    "                          [0, 0.1], #xE\n",
    "                          [-0.1, 0], #xI\n",
    "                          [1, 365], # tau\n",
    "                          [-10, 100]]# h0\n",
    "\n",
    "        for i in range(len(initial_params)):\n",
    "            if initial_params[i] is None:\n",
    "                initial_params[i] = np.nan\n",
    "            if initial_params[i] < boundaries[i][0] or np.isnan(initial_params[i]):\n",
    "                initial_params[i] = (boundaries[i][0]+boundaries[i][1]) / 2\n",
    "            if initial_params[i] > boundaries[i][1] or np.isnan(initial_params[i]):\n",
    "                initial_params[i] = (boundaries[i][0]+boundaries[i][1]) / 2\n",
    "        \n",
    "            \n",
    "        \n",
    "        boundaries = np.array(boundaries).T\n",
    "        \n",
    "        #def get_model_estimates(params, InSAR_t, h_t, t0_dt, tmin_dt, tmax_dt, t0_yf, tmin_yf, tmax_yf, etmgeg):\n",
    "        def model(t, *params, InSAR_t=InSAR_t, h_t=t_data, t0_dt=t0_dt, tmin_dt=tmin_dt, tmax_dt=tmax_dt, t0_yf=t0_yf, tmin_yf=tmin_yf, tmax_yf=tmax_yf,\n",
    "                  etmgeg=etmgeg, return_xIfrac=False):#, tau=tau):\n",
    "            xP = params[0]\n",
    "            xE = params[1]\n",
    "            xI = params[2]\n",
    "            #tau = params[3]\n",
    "            #h0 = params[3]\n",
    "            tau = params[3]\n",
    "            if len(h_t) > 0:\n",
    "                h0 = params[4]\n",
    "            else:\n",
    "                h0 = 0\n",
    "            \n",
    "            InSAR_h = []\n",
    "            h_h = []\n",
    "            ndays = (tmax_dt-tmin_dt).days\n",
    "            \n",
    "            R_array = np.zeros((ndays, ))\n",
    "            I_array = np.zeros((ndays, ))\n",
    "            t_array = np.zeros((ndays, ))\n",
    "            t0_idx = None\n",
    "            \n",
    "            for day in range(ndays):\n",
    "                for backwards_day in range(int(tau)+1):\n",
    "                    cur_day = tmin_dt + dt.timedelta(days=day-backwards_day)\n",
    "                    try:\n",
    "                        Pt = etmgeg[cur_day][0]\n",
    "                        Et = etmgeg[cur_day][1]\n",
    "                        if backwards_day == int(tau):\n",
    "                            R_array[day] += (Pt*xP - Et*xE) * (tau % 1)\n",
    "                        else:\n",
    "                            R_array[day] += (Pt*xP - Et*xE)\n",
    "                            \n",
    "                    except KeyError:\n",
    "                        continue\n",
    "            \n",
    "                if R_array[day] <= 0:\n",
    "                    I_array[day] = I_array[day-1] + xI\n",
    "                else:\n",
    "                    I_array[day] = I_array[day-1]\n",
    "                \n",
    "                date = tmin_dt + dt.timedelta(days=day)\n",
    "                t_array[day] = get_yearfrac('{:0>4d}-{:0>2d}-{:0>2d}'.format(date.year, date.month, date.day))\n",
    "                if date == t0_dt:\n",
    "                    t0_idx = day\n",
    "            \n",
    "            t_lst = list(t_array)\n",
    "            if t0_idx is None:\n",
    "                raise ValueError('t0 < tmax!')\n",
    "            \n",
    "            xIfrac = sum(1*R_array<=0)/R_array.shape[0]\n",
    "            \n",
    "            R_array -= R_array[t0_idx]\n",
    "            I_array -= I_array[t0_idx]\n",
    "            \n",
    "            for t in InSAR_t:\n",
    "                yft = get_yearfrac(t)\n",
    "                tidx = t_lst.index(yft)\n",
    "                InSAR_h.append(R_array[tidx]+I_array[tidx])\n",
    "            \n",
    "            for t in h_t:\n",
    "                if t < tmin_yf:\n",
    "                    elev = h0 + R_array[0] + I_array[0] + (dt.datetime.strptime(retrieve_epoch(t), '%Y-%m-%d') - tmin_dt).days * xI * xIfrac\n",
    "                else:\n",
    "                    tidx = t_lst.index(t)\n",
    "                    elev = h0 + R_array[tidx]+I_array[tidx]\n",
    "                h_h.append(elev)\n",
    "    \n",
    "            y_est = []        \n",
    "            for i in range(len(h_h)):\n",
    "                y_est.append(h_h[i])\n",
    "            for i in range(len(InSAR_t)):\n",
    "                y_est.append(InSAR_h[i])\n",
    "            \n",
    "            if return_xIfrac:\n",
    "                return xIfrac\n",
    "            \n",
    "            return y_est\n",
    "    \n",
    "        if len(y_obs) <= 5:\n",
    "            return no_insar_model()\n",
    "        \n",
    "        try:\n",
    "            popt, pcov = curve_fit(model,\n",
    "                                   xdata=t_obs,\n",
    "                                   ydata=y_obs,\n",
    "                                   p0=initial_params,\n",
    "                                   sigma=s_obs,\n",
    "                                   bounds=boundaries,\n",
    "                                   method=\"trf\",\n",
    "                                   absolute_sigma=True,\n",
    "                                   xtol=1e-6)\n",
    "            \n",
    "            stds = np.sqrt(np.diag(pcov))\n",
    "            if len(t_data) > 0:\n",
    "                xIfrac = model(t_obs, popt[0], popt[1], popt[2], popt[3], popt[4], return_xIfrac=True)\n",
    "            else:\n",
    "                xIfrac = model(t_obs, popt[0], popt[1], popt[2], popt[3], return_xIfrac=True)\n",
    "                popt = np.append(popt, [-999])\n",
    "                stds = np.append(stds, [-999])\n",
    "                pcov_ = np.ones((5, 5)) * -999\n",
    "                pcov_[:-1, :-1] = pcov[:, :]\n",
    "                pcov = pcov_[:, :]\n",
    "        except (ValueError, RuntimeError):  # Model does not converge!\n",
    "            #print(\"Warning!! Optimal parameters not found, model did not converge!\")\n",
    "            #popt = [-999] * 5\n",
    "            #stds = [-999] * 5\n",
    "            #xIfrac = -999\n",
    "            #pcov = np.ones((5, 5)) * -999\n",
    "            return no_insar_model()\n",
    "        \n",
    "        \n",
    "        return [popt, stds, max(t_obs) - min(t_obs), xIfrac, pcov]\n",
    "        \n",
    "    else: # no InSAR --> linear model \n",
    "        \n",
    "        return no_insar_model()\n",
    "    \n",
    "    \n",
    "def collect_all_models(EUPs, t0, tmin, tmax):\n",
    "    all_models = {}\n",
    "    for EUP in EUPs.keys():\n",
    "        all_models[EUP] = fit_SPAMS_extended(EUPs[EUP], t0, tmin, tmax)\n",
    "    return all_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf299f-a905-4f02-b441-f357462febc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    EUP_f = open(f'Data/EUPs/{file}.txt')\n",
    "    raw = EUP_f.read()\n",
    "    EUP_f.close()\n",
    "    raw_EUP = raw.replace('nan', \"'nan'\").replace('array', 'np.array')\n",
    "    EUP = eval(raw_EUP)\n",
    "\n",
    "    if EUP['InSAR'] != -999:\n",
    "        for keykey in [k for k in EUP['InSAR'].keys() if k[0] == 'h']:\n",
    "            for d in range(len(EUP['InSAR'][keykey])):\n",
    "                if EUP['InSAR'][keykey][d] == 'nan':\n",
    "                    EUP['InSAR'][keykey][d] = np.nan\n",
    "\n",
    "    \n",
    "    y = []\n",
    "    y_insar = []\n",
    "    t = []\n",
    "    t_insar = []\n",
    "    s = []\n",
    "    s_insar = []\n",
    "    min_t = None\n",
    "    max_t = None\n",
    "\n",
    "    if EUP['EUP_type'] == 'brp':\n",
    "        for key in ['100_AHN1', '100_AHN2', '100_AHN3', '100_AHN4', '_ThMD']:\n",
    "            if EUP[f't{key}'] not in [-999, '']:\n",
    "                y.append(EUP[f'h{key}'])\n",
    "                t.append(EUP[f't{key}'])\n",
    "                s.append(max(EUP[f's{key}'], 0.001))\n",
    "\n",
    "        if EUP['InSAR'] != -999:\n",
    "            keys = [key[1:] for key in EUP['InSAR'].keys() if key[0] == 't']\n",
    "            for key in keys:\n",
    "                for t_ in range(len(EUP['InSAR'][f't{key}'])):\n",
    "                    if not np.isnan(EUP['InSAR'][f'h{key}'][t_]):\n",
    "                        t_insar.append(EUP['InSAR'][f't{key}'][t_])\n",
    "                        y_insar.append(EUP['InSAR'][f'h{key}'][t_])\n",
    "                        s_insar.append(0.01)\n",
    "\n",
    "    else:\n",
    "        for key in ['100_AHN1', '100_AHN2', '100_AHN3', '100_AHN4', '_ThMD']:\n",
    "            if EUP[f't{key}'] not in [-999, '']:\n",
    "                y.append(EUP[f'h{key}'])\n",
    "                t.append(EUP[f't{key}'])\n",
    "                s.append(EUP[f's{key}'])\n",
    "    full_y = []\n",
    "    full_t = []\n",
    "    full_s = []\n",
    "    for d in range(len(y)):\n",
    "        full_y.append(y[d])\n",
    "        full_t.append(get_yearfrac(t[d]))\n",
    "        full_s.append(s[d])\n",
    "    for d in range(len(y_insar)):\n",
    "        full_y.append(y_insar[d]+EUP['h0'])\n",
    "        full_t.append(get_yearfrac(t_insar[d]))\n",
    "        full_s.append(s_insar[d])\n",
    "    return full_y, full_t, full_s\n",
    "\n",
    "def plot_model(file):\n",
    "    EUP_f = open(f'Data/EUPs/{file}.txt')\n",
    "    raw = EUP_f.read()\n",
    "    EUP_f.close()\n",
    "    raw_EUP = raw.replace('nan', \"'nan'\").replace('array', 'np.array')\n",
    "    EUP = eval(raw_EUP)\n",
    "\n",
    "    if EUP['InSAR'] != -999:\n",
    "        for keykey in [k for k in EUP['InSAR'].keys() if k[0] == 'h']:\n",
    "            for d in range(len(EUP['InSAR'][keykey])):\n",
    "                if EUP['InSAR'][keykey][d] == 'nan':\n",
    "                    EUP['InSAR'][keykey][d] = np.nan\n",
    "    tt = []\n",
    "    h = []\n",
    "    cur_t = t_eval_min\n",
    "    while get_yearfrac('{:0>4d}-{:0>2d}-{:0>2d}'.format(cur_t.year, cur_t.month, cur_t.day)) < tmax_yf:\n",
    "        tt.append('{:0>4d}-{:0>2d}-{:0>2d}'.format(cur_t.year, cur_t.month, cur_t.day))\n",
    "        cur_t += dt.timedelta(days=1)\n",
    "\n",
    "    xI = EUP['xI']\n",
    "    xP = EUP['xP']\n",
    "    xE = EUP['xE']\n",
    "    h0 = EUP['h0']\n",
    "    tau = EUP['tau']\n",
    "\n",
    "    ndays = (tmax_dt-tmin_dt).days\n",
    "\n",
    "    R_array = np.zeros((ndays, ))\n",
    "    I_array = np.zeros((ndays, ))\n",
    "    t_array = np.zeros((ndays, ))\n",
    "    t0_idx = None\n",
    "\n",
    "    for day in range(ndays):\n",
    "        for backwards_day in range(int(tau)+1):\n",
    "            cur_day = tmin_dt + dt.timedelta(days=day-backwards_day)\n",
    "            try:\n",
    "                Pt = etmgeg[cur_day][0]\n",
    "                Et = etmgeg[cur_day][1]\n",
    "                if backwards_day == int(tau):\n",
    "                    R_array[day] += (Pt*xP - Et*xE) * (tau % 1)\n",
    "                else:\n",
    "                    R_array[day] += (Pt*xP - Et*xE)\n",
    "\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        if R_array[day] <= 0:\n",
    "            I_array[day] = I_array[day-1] + xI\n",
    "        else:\n",
    "            I_array[day] = I_array[day-1]\n",
    "\n",
    "        date = tmin_dt + dt.timedelta(days=day)\n",
    "        t_array[day] = get_yearfrac('{:0>4d}-{:0>2d}-{:0>2d}'.format(date.year, date.month, date.day))\n",
    "        if date == t0_dt:\n",
    "            t0_idx = day\n",
    "\n",
    "    t_lst = list(t_array)\n",
    "    if t0_idx is None:\n",
    "        raise ValueError('t0 > tmax!')\n",
    "\n",
    "    xIfrac = sum(1*R_array<=0)/R_array.shape[0]\n",
    "\n",
    "    R_array -= R_array[t0_idx]\n",
    "    I_array -= I_array[t0_idx]\n",
    "\n",
    "\n",
    "    for t in tt:\n",
    "        if get_yearfrac(t) < tmin_yf:\n",
    "            elev = h0 + R_array[0] + I_array[0] + (dt.datetime.strptime(t, '%Y-%m-%d') - tmin_dt).days * xI * xIfrac\n",
    "        else:\n",
    "            tidx = t_lst.index(get_yearfrac(t))\n",
    "            elev = h0 + R_array[tidx]+I_array[tidx]\n",
    "        h.append(elev)\n",
    "\n",
    "    return [get_yearfrac(t) for t in tt], h\n",
    "\n",
    "def convert_value_to_hex(value, vmin=-10, vmax=10):\n",
    "    pct = min(max((value - vmin) / (vmax - vmin), 0), 1)\n",
    "    clr = (int(255 * min(1, 2 - 2 * pct)), \n",
    "           int(255 * (1 - 2 * abs(0.5 - pct))), \n",
    "           int(255 * min(1, 2 * pct)))\n",
    "    #clr = (int(255 * max(0, (1 - 2 * pct))), 0, (int(255 * max(2*pct - 1, 0))))\n",
    "    hx = '#'\n",
    "    for c in clr:\n",
    "        hx += \"{:0>2s}\".format(hex(c)[2:])\n",
    "    return hx\n",
    "\n",
    "def map_index(EUP):\n",
    "    \"\"\"\n",
    "    This function maps the index of the gpkg EUP to the txt EUP, as these are not the same:\n",
    "    the gpkg maintains the original numbering before ~50000 EUPs were removed, the txt EUP\n",
    "    starts counting from 0 without gaps\n",
    "    :param EUP: the gpkg index of the EUP\n",
    "    :return: the txt index of the EUP\n",
    "    \"\"\"\n",
    "    keys = list(gpkg_data.keys())\n",
    "    print(keys.index(EUP))\n",
    "    return keys.index(EUP)\n",
    "\n",
    "def map_vis_index(EUP):\n",
    "    \"\"\"\n",
    "    This function maps the index of the gpkg EUP to the visualisation EUP, as these are not the same:\n",
    "    the gpkg maintains the original numbering before ~50000 EUPs were removed, the visualisation EUP\n",
    "    starts counting from 0 without gaps and has EUPs with no data removed\n",
    "    :param EUP: the gpkg index of the EUP\n",
    "    :return: the vis index of the EUP\n",
    "    \"\"\" \n",
    "    vis_id = None\n",
    "    for att_id in vis_data.keys():\n",
    "        if gpkg_data[EUP]['crd_dec'] == vis_data[att_id]['crd_dec']:\n",
    "            vis_id = att_id\n",
    "            break\n",
    "    return vis_id\n",
    "\n",
    "def convert_gpkg_to_wgs84(gpkg):\n",
    "    for EUP in gpkg.keys():\n",
    "        gpkg[EUP]['crd_wgs'] = convert_rd_to_wgs84(gpkg_data[EUP]['crd_dec'][0])\n",
    "    return gpkg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767bfe09-7182-46ce-8aed-409d052f4a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpkg_data = convert_gpkg_to_wgs84(gpkg_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8fe86c-db24-4788-9f8a-73e30acd36e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14406c6f-8123-4879-aaba-95bf4e28ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = widgets.Output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc7f6a4-412c-4c51-b339-ec45cf53cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_overview(plot='xI'):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        display(main_plot)\n",
    "    if plot != 'Select':\n",
    "        vdir = {'xI': [-10, 10],\n",
    "                's_xI': [0, 10],\n",
    "                'h0': [-8, 2],\n",
    "                's_h0': [0, 10]}\n",
    "                    \n",
    "        with output:\n",
    "            CarDB = basemap_to_tiles(basemaps.CartoDB.Positron)\n",
    "            CarDB.base = True\n",
    "            CarDB.name = 'Map (CartoDB)'\n",
    "            \n",
    "            m = Map(center=list(np.mean(np.array(aoi_wgs_poly), axis=0)), zoom=12, min_zoom=12, close_popup_on_click=False , layers = [CarDB])\n",
    "\n",
    "            AoI = LayerGroup(name = 'AoI')\n",
    "            eup_groups = []\n",
    "            eup_groups.append(LayerGroup())           \n",
    "\n",
    "            line = Polyline(locations = aoi_wgs_poly,\n",
    "                        weight = 2,\n",
    "                        color= '#000000', linestyle='--', fill=False) \n",
    "\n",
    "            highlighter = Polyline(locations = [[0, 0], [0, 0]],\n",
    "                                   weight=3,\n",
    "                                   color='#5abf4d', fill=False, name='Selection')\n",
    "            \n",
    "            AoI.add_layer(line)\n",
    "\n",
    "            def update_data(b):\n",
    "                with out4:\n",
    "                    print(f'Should remove {b.description.split(\" \")[-1]}')\n",
    "            \n",
    "            def plot_data(EUP, m):\n",
    "                def button_click(**kwargs):\n",
    "                    clear_output(wait=True)\n",
    "                    txt_eup = map_index(EUP)\n",
    "                    vis_eup = map_vis_index(EUP)                        \n",
    "                    t, h = plot_model(txt_eup)\n",
    "                    y, yt, ys = get_data(txt_eup)\n",
    "                    EUP_f = open(f'Data/EUPs/{txt_eup}.txt')\n",
    "                    raw = EUP_f.read()\n",
    "                    EUP_f.close()\n",
    "                    raw_EUP = raw.replace('nan', \"'nan'\").replace('array', 'np.array')\n",
    "                    data = eval(raw_EUP)\n",
    "                    m.remove_layer(m.layers[-1])\n",
    "                    highlighter = Polyline(locations = convert_rd_to_wgs84(gpkg_data[EUP]['crd_dec'][0]),\n",
    "                                weight=3,\n",
    "                                color='#5abf4d', fill=False, name='Selection')\n",
    "                    m.add_layer(highlighter)\n",
    "                    with out1:\n",
    "                        clear_output(wait=True)\n",
    "                        plt.figure(figsize=(11,5))\n",
    "                        plt.plot(t, h, label='Model fit')\n",
    "                        plt.errorbar(yt, y, yerr=ys, fmt='o', ecolor='r', label='Observations')\n",
    "                        plt.grid(True)\n",
    "                        plt.xlabel('Time [y]')\n",
    "                        plt.ylabel('Elevation [m-NAP1997]')\n",
    "                        plt.legend()\n",
    "                        plt.show()\n",
    "                    with out3:\n",
    "                        clear_output(wait=True)\n",
    "                        if vis_eup is None:\n",
    "                            print('This spatial unit was filtered out of the resulting dataset due to a failed model estimate')\n",
    "                        else:\n",
    "                            if data['model_mode'] == \"SPAMS\":\n",
    "                                print(f'''GPKG spatial unit {EUP} (TXT {txt_eup}, VIS {vis_eup})\n",
    "    ----------\n",
    "    Estimated parameters ({data['model_mode'] + ('-extended' if data['model_mode'] == 'SPAMS' else '')} model)\n",
    "    \n",
    "    Elevation (1 Jan 2015): {round(vis_data[vis_eup]['h2015_m'], 3)}±{round(vis_data[vis_eup]['sigma_h2015_m'], 3)} m - NAP\n",
    "    XI (irreversible): {round(vis_data[vis_eup]['xI_mpy']*1000, 1)}±{round(vis_data[vis_eup]['sigma_xI_mpy']*1000, 1)} mm/y ({round(vis_data[vis_eup]['xI_frac_1000pct']*1000, 1)}% active)\n",
    "    XP (precipitation): {round(vis_data[vis_eup]['xP_mpmm']*1000, 3)}±{round(vis_data[vis_eup]['sigma_xP_mpmm']*1000, 3)} mm/mm\n",
    "    XE (evapotranspiration): {round(vis_data[vis_eup]['xE_mpmm']*1000, 3)}±{round(vis_data[vis_eup]['sigma_xE_mpmm']*1000, 3)} mm/mm\n",
    "    Tau (delay): {round(vis_data[vis_eup]['tau_kd']*1000, 1)}±{round(vis_data[vis_eup]['sigma_tau_kd']*1000, 1)} days\n",
    "    \n",
    "    ----------\n",
    "    Derived variables\n",
    "    \n",
    "    Mean Yearly Irreversible Subsidence: {round(vis_data[vis_eup]['mean_yearly_irreversible_subsidence_MYIS_mpy']*1000, 1)}±{round(vis_data[vis_eup]['sigma_MYIS_mpy']*1000, 1)} mm/y \n",
    "    Observation timespan: {round(vis_data[vis_eup]['timespan_ky']*1000, 1)} y\n",
    "    # observations: {int(1000*vis_data[vis_eup]['n_observations_1000'])}\n",
    "    Cumulative irreversible subsidence since 2000: {round(vis_data[vis_eup]['cum_irreversible_subsidence_since_2000_m'] * 1000, 1)} mm\n",
    "    \n",
    "    ----------\n",
    "    Covariances\n",
    "    cov(h0, xI): {round(vis_data[vis_eup]['rho_h2015_xI_1000']*1000 * vis_data[vis_eup]['sigma_h2015_m'] * vis_data[vis_eup]['sigma_xI_mpy']*1000, 3)}\n",
    "    cov(h0, xP): {round(vis_data[vis_eup]['rho_h2015_xP_1000']*1000 * vis_data[vis_eup]['sigma_h2015_m'] * vis_data[vis_eup]['sigma_xP_mpmm']*1000, 3)}\n",
    "    cov(h0, xE): {round(vis_data[vis_eup]['rho_h2015_xE_1000']*1000 * vis_data[vis_eup]['sigma_h2015_m'] * vis_data[vis_eup]['sigma_xE_mpmm']*1000, 3)}\n",
    "    cov(h0, tau): {round(vis_data[vis_eup]['rho_h2015_tau_1000']*1000 * vis_data[vis_eup]['sigma_h2015_m'] * vis_data[vis_eup]['sigma_tau_kd']*1000, 3)}\n",
    "    cov(xI, xE): {round(vis_data[vis_eup]['rho_xI_xE_1000']*1000 * vis_data[vis_eup]['sigma_xI_mpy']*1000 * vis_data[vis_eup]['sigma_xE_mpmm']*1000, 3)}\n",
    "    cov(xI, xP): {round(vis_data[vis_eup]['rho_xI_xP_1000']*1000 * vis_data[vis_eup]['sigma_xI_mpy']*1000 * vis_data[vis_eup]['sigma_xP_mpmm']*1000, 3)}\n",
    "    cov(xI, tau): {round(vis_data[vis_eup]['rho_xI_tau_1000']*1000 * vis_data[vis_eup]['sigma_xI_mpy']*1000 * vis_data[vis_eup]['sigma_tau_kd']*1000, 3)}\n",
    "    cov(xP, xE): {round(vis_data[vis_eup]['rho_xP_xE_1000']*1000 * vis_data[vis_eup]['sigma_xP_mpmm']*1000 * vis_data[vis_eup]['sigma_xE_mpmm']*1000, 3)}\n",
    "    cov(xP, tau): {round(vis_data[vis_eup]['rho_xP_tau_1000']*1000 * vis_data[vis_eup]['sigma_xP_mpmm']*1000 * vis_data[vis_eup]['sigma_tau_kd']*1000, 3)}\n",
    "    cov(xE, tau): {round(vis_data[vis_eup]['rho_xE_tau_1000']*1000 * vis_data[vis_eup]['sigma_xE_mpmm']*1000 * vis_data[vis_eup]['sigma_tau_kd']*1000, 3)}\n",
    "    \n",
    "    ---------\n",
    "    Statistics\n",
    "    \n",
    "    Significant irreversible subsidence detected: {vis_data[vis_eup]['significant_irreversible_subsidence_detected_2s_1000']*1000} (2 sigma) / {vis_data[vis_eup]['significant_irreversible_subsidence_detected_3s_1000']*1000} (3 sigma)\n",
    "    OMT sustained: {vis_data[vis_eup]['OMT_sust_2s_1000']*1000} (2 sigma) / {vis_data[vis_eup]['OMT_sust_3s_1000']*1000} (3 sigma)\n",
    "    Discriminatory power: {round(vis_data[vis_eup]['discriminatory_power_2s_1000pct']*1000, 2)}% (2 sigma) / {round(vis_data[vis_eup]['discriminatory_power_3s_1000pct']*1000, 2)}% (3 sigma)\n",
    "    Minimum Detectable Displacement (80% confidence): {round(vis_data[vis_eup]['MDD_80p_mpy']*1000, 2)} mm/y\n",
    "    \n",
    "    ''')\n",
    "                            else: \n",
    "                                print(f'''GPKG spatial unit {EUP} (TXT {txt_eup}, VIS {vis_eup})\n",
    "----------\n",
    "Estimated parameters ({data['model_mode'] + ('-extended' if data['model_mode'] == 'SPAMS' else '')} model)\n",
    "\n",
    "Elevation (1 Jan 2015): {round(vis_data[vis_eup]['h2015_m'], 3)}±{round(vis_data[vis_eup]['sigma_h2015_m'], 3)} m - NAP\n",
    "XI (irreversible): {round(vis_data[vis_eup]['xI_mpy']*1000, 1)}±{round(vis_data[vis_eup]['sigma_xI_mpy']*1000, 1)} mm/y ({round(vis_data[vis_eup]['xI_frac_1000pct']*1000, 1)}% active)\n",
    "\n",
    "----------\n",
    "Derived variables\n",
    "\n",
    "Mean Yearly Irreversible Subsidence: {round(vis_data[vis_eup]['mean_yearly_irreversible_subsidence_MYIS_mpy']*1000, 1)}±{round(vis_data[vis_eup]['sigma_MYIS_mpy']*1000, 1)} mm/y \n",
    "Observation timespan: {round(vis_data[vis_eup]['timespan_ky']*1000, 1)} y\n",
    "# observations: {int(1000*vis_data[vis_eup]['n_observations_1000'])}\n",
    "Cumulative irreversible subsidence since 2000: {round(vis_data[vis_eup]['cum_irreversible_subsidence_since_2000_m'] * 1000, 1)} mm\n",
    "\n",
    "----------\n",
    "Covariances\n",
    "cov(h0, xI): {round(vis_data[vis_eup]['rho_h2015_xI_1000']*1000 * vis_data[vis_eup]['sigma_h2015_m'] * vis_data[vis_eup]['sigma_xI_mpy']*1000, 3)}\n",
    "\n",
    "---------\n",
    "Statistics\n",
    "\n",
    "Significant irreversible subsidence detected: {vis_data[vis_eup]['significant_irreversible_subsidence_detected_2s_1000']*1000} (2 sigma) / {vis_data[vis_eup]['significant_irreversible_subsidence_detected_3s_1000']*1000} (3 sigma)\n",
    "OMT sustained: {vis_data[vis_eup]['OMT_sust_2s_1000']*1000} (2 sigma) / {vis_data[vis_eup]['OMT_sust_3s_1000']*1000} (3 sigma)\n",
    "Discriminatory power: {round(vis_data[vis_eup]['discriminatory_power_2s_1000pct']*1000, 2)}% (2 sigma) / {round(vis_data[vis_eup]['discriminatory_power_3s_1000pct']*1000, 2)}% (3 sigma)\n",
    "Minimum Detectable Displacement (80% confidence): {round(vis_data[vis_eup]['MDD_80p_mpy']*1000, 2)} mm/y\n",
    "\n",
    "''')\n",
    "                        #print(f'GPKG EUP {EUP} (TXT {txt_eup}, VIS {vis_eup})')\n",
    "                        #print(f\"Mean Yearly Irreversible Subsidence: {round(data['xI'] * data['xI_frac'] * 365.2425 * 1000, 1)}±{round(data['s_xI'] * data['xI_frac'] * 365.2425 * 1000, 1)} mm/y\")\n",
    "                        #print(f\"Elevation (1 Jan 2015): {round(data['h0'], 3)}±{round(data['s_h0'], 3)} m-NAP1997\")\n",
    "                        \n",
    "                    #with out3:\n",
    "                    #    clear_output(wait=True)\n",
    "                        #if data['t_ThMD'] not in ['', -999]:\n",
    "                        #    thmd_button = widgets.Button(description='Remove ThMD')\n",
    "                        #else:\n",
    "                        #    thmd_button = widgets.Button(description='Remove ThMD', disabled=True)\n",
    "                        #thmd_button.on_click(update_data)\n",
    "                        #if data['t80_AHN1'] not in ['', -999]:\n",
    "                        #    ahn1_button = widgets.Button(description='Remove AHN1')\n",
    "                        #else:\n",
    "                        #    ahn1_button = widgets.Button(description='Remove AHN1', disabled=True)\n",
    "                        #ahn1_button.on_click(update_data)\n",
    "                        #if data['t80_AHN2'] not in ['', -999]:\n",
    "                        #    ahn2_button = widgets.Button(description='Remove AHN2')\n",
    "                        #else:\n",
    "                        #    ahn2_button = widgets.Button(description='Remove AHN2', disabled=True)\n",
    "                        #ahn2_button.on_click(update_data)\n",
    "                        #if data['t80_AHN3'] not in ['', -999]:\n",
    "                        #    ahn3_button = widgets.Button(description='Remove AHN3')\n",
    "                        #else:\n",
    "                        #    ahn3_button = widgets.Button(description='Remove AHN3', disabled=True)\n",
    "                        #ahn3_button.on_click(update_data)\n",
    "                        #if data['t80_AHN4'] not in ['', -999]:\n",
    "                        #    ahn4_button = widgets.Button(description='Remove AHN4')\n",
    "                        #else:\n",
    "                        #    ahn4_button = widgets.Button(description='Remove AHN4', disabled=True)\n",
    "                        #ahn4_button.on_click(update_data)\n",
    "                        #if data['InSAR'] not in ['', -999]:\n",
    "                        #    insar_button = widgets.Button(description='Remove InSAR')\n",
    "                        #else:\n",
    "                        #    insar_button = widgets.Button(description='Remove InSAR', disabled=True)\n",
    "                        #insar_button.on_click(update_data)\n",
    "                        #eup_button = widgets.Button(description='Remove EUP')\n",
    "                        #display(eup_button)\n",
    "                        #display(thmd_button)\n",
    "                        #display(ahn1_button)\n",
    "                        #display(ahn2_button)\n",
    "                        #display(ahn3_button)\n",
    "                        #display(ahn4_button)\n",
    "                        #display(insar_button)\n",
    "                    #with out4:\n",
    "                    #    print(f'Analysing GPKG EUP {EUP} (TXT {txt_eup})')\n",
    "                    with out2:\n",
    "                        print(f'''GPKG spatial unit {EUP} (TXT {txt_eup}, VIS {vis_eup})''')\n",
    "                    with out4:\n",
    "                        print(f'''GPKG spatial unit {EUP} (TXT {txt_eup}, VIS {vis_eup})''') \n",
    "                        \n",
    "                return button_click\n",
    "            out1 = widgets.Output()\n",
    "            out2 = widgets.Output()\n",
    "            out3 = widgets.Output()\n",
    "            out4 = widgets.Output()\n",
    "        \n",
    "            tab = widgets.Tab(children = [out1, out2])\n",
    "            tab.set_title(0, 'Estimated Model')\n",
    "            tab.set_title(1, 'Unused')\n",
    "\n",
    "            tab2 = widgets.Tab(children = [out3, out4])\n",
    "            tab2.set_title(0, 'Parameters')\n",
    "            tab2.set_title(1, 'Unused')\n",
    "            \n",
    "            m.add(FullScreenControl())\n",
    "            m.add_layer(highlighter)\n",
    "\n",
    "            display(tab2)\n",
    "            display(tab)\n",
    "            display(m)\n",
    "            ct = 0\n",
    "            with out1:\n",
    "                clear_output(wait=True)\n",
    "                print('Loading spatial units...')\n",
    "                print(f'{ct}/{len(gpkg_data.keys())}')\n",
    "            for EUP in list(gpkg_data.keys()):\n",
    "                try:\n",
    "                    if plot in ['xI', 's_xI']:\n",
    "                        val = gpkg_data[EUP][plot] * gpkg_data[EUP]['xI_frac'] * 365.2425 * 1000 # mm/y\n",
    "                    elif plot in ['h0']:\n",
    "                        val = gpkg_data[EUP][plot] # m\n",
    "                    elif plot in ['s_h0']:\n",
    "                        val = gpkg_data[EUP][plot] * 100 # cm\n",
    "                    else:\n",
    "                        val = gpkg_data[EUP][plot] * 1000\n",
    "                    polygon = Polygon(\n",
    "                        locations = gpkg_data[EUP]['crd_wgs'],\n",
    "                        weight=0,\n",
    "                        fill_color=convert_value_to_hex(val, vdir[plot][0], vdir[plot][1]),\n",
    "                        fill_opacity=1)\n",
    "                    polygon.on_click(plot_data(EUP, m=m))\n",
    "                    eup_groups[-1].add_layer(polygon)\n",
    "                    ct += 1\n",
    "                    if ct % MAX_GROUP_SIZE == 0:\n",
    "                        with out1:\n",
    "                            clear_output(wait=True)\n",
    "                            print('Loading spatial units...')\n",
    "                            print(f'{ct}/{len(gpkg_data.keys())}')\n",
    "                        m.add_layer(eup_groups[-1])\n",
    "                        eup_groups.append(LayerGroup())\n",
    "                except TypeError:\n",
    "                    pass\n",
    "            if ct % MAX_GROUP_SIZE != 0:      \n",
    "                m.add_layer(eup_groups[-1])\n",
    "                with out1:\n",
    "                    clear_output(wait=True)\n",
    "                    print('Loaded spatial units!')\n",
    "                    print(f'Click to analyse a spatial unit')\n",
    "                with out2:\n",
    "                    clear_output(wait=True)\n",
    "                    print('Loaded spatial units!')\n",
    "                    print(f'Click to analyse a spatial unit')\n",
    "                with out3:\n",
    "                    clear_output(wait=True)\n",
    "                    print('Loaded spatial units!')\n",
    "                    print(f'Click to analyse a spatial unit')\n",
    "                with out4:\n",
    "                    clear_output(wait=True)\n",
    "                    print('Loaded spatial units!')\n",
    "                    print(f'Click to analyse a spatial unit')\n",
    "            m.add_layer(AoI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff631c-d891-4abe-acbf-12a393cda768",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_drop = widgets.Dropdown(\n",
    "        options=['Select', 'xI', 's_xI', 'h0', 's_h0'],\n",
    "        value='Select',\n",
    "        description='Parameter:',\n",
    "        disabled=False,\n",
    "    )\n",
    "main_plot = interactive(plot_overview, plot=plot_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a6f7d-7a9c-4a4f-8b0a-fa1406fdcccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "button = widgets.Button(description=\"Run Me!\")\n",
    "\n",
    "\n",
    "display(button, output)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        display(main_plot)\n",
    "        \n",
    "button.on_click(on_button_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5cf814-df6e-49b6-8b56-126b32c8901b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba27175-7d34-4e17-b4cc-8e079fdae31e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
